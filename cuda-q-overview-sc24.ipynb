{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n",
    "# SPDX-License-Identifier: LicenseRef-NvidiaProprietary\n",
    "#\n",
    "# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual\n",
    "# property and proprietary rights in and to this material, related\n",
    "# documentation and any modifications thereto. Any use, reproduction,\n",
    "# disclosure or distribution of this material and related documentation\n",
    "# without an express license agreement from NVIDIA CORPORATION or\n",
    "# its affiliates is strictly prohibited."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accelerating Quantum Computing: A Step-by-Step Guide to Expanding Simulation\u000bCapabilities and Enabling Interoperability of Quantum Hardware\n",
    "  \n",
    "## Overview of methods of accelerating quantum simulation with GPUs.\n",
    "\n",
    "* Introduction to CUDA-Q through three Hello World examples using `sample` and `observe` calls.  \n",
    "* Guide to different simulation backends for executing quantum circuits, emphasizing a variety of patterns of parallelization: \n",
    "    * statevector memory over multiple processors\n",
    "    * circuit sampling over multiple processors\n",
    "    * Hamiltonian batching\n",
    "    * circuit cutting\n",
    "* Live-demo installation and demonstration of how to access the NV Quantum Cloud.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hello World Examples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudaq\n",
    "from cudaq import spin\n",
    "from typing import List\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 00:534 11:466 }\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 1 - sampling a circuit\n",
    "\n",
    "##############################################################\n",
    "#  1. Select a backend for kernel execution\n",
    "cudaq.set_target(\"qpp-cpu\")\n",
    "##############################################################\n",
    "\n",
    "##############################################################\n",
    "# 2. Define a kernel function \n",
    "@cudaq.kernel\n",
    "def kernel(qubit_count: int):\n",
    "    # Allocate our `qubit_count` to the kernel.\n",
    "    qvector = cudaq.qvector(qubit_count)\n",
    "\n",
    "    # Apply a Hadamard gate to the qubit indexed by 0.\n",
    "    h(qvector[0])\n",
    "    # Apply a Controlled-X gate between qubit 0 (acting as the control)\n",
    "    # and each of the remaining qubits.  \n",
    "    for i in range(1, qubit_count):\n",
    "        x.ctrl(qvector[0], qvector[i])\n",
    "\n",
    "    # Measure the qubits\n",
    "    # If we don't specify measurements, all qubits are measured in\n",
    "    # the Z-basis by default.\n",
    "    mz(qvector)\n",
    "\n",
    "##############################################################\n",
    "# 3. Call the kernel function with the variable qubit_count set to 2 and sample the outcomes\n",
    "qubit_count = 2\n",
    "result = cudaq.sample(kernel, qubit_count, shots_count=1000)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<psi|H|psi> = -0.030000000000000027\n"
     ]
    }
   ],
   "source": [
    "# Example 2 - Expectation value calculations\n",
    "\n",
    "# Define a quantum kernel function.\n",
    "@cudaq.kernel\n",
    "def kernel(qubit_count: int):\n",
    "    # Allocate our `qubit_count` to the kernel.\n",
    "    qvector = cudaq.qvector(qubit_count)\n",
    "\n",
    "    # Apply a Hadamard gate to the qubit indexed by 0.\n",
    "    h(qvector[0])\n",
    "    # Apply a Controlled-X gate between qubit 0 (acting as the control)\n",
    "    # and each of the remaining qubits.  \n",
    "    for i in range(1, qubit_count):\n",
    "        x.ctrl(qvector[0], qvector[i])\n",
    "\n",
    "# Define a Hamiltonian in terms of Pauli Spin operators.\n",
    "hamiltonian = spin.z(0) + 2*spin.y(1) - spin.x(0) * spin.z(1)\n",
    "\n",
    "# Compute the expectation value given the state prepared by the kernel.\n",
    "result = cudaq.observe(kernel, hamiltonian, qubit_count, shots_count = 1000).expectation()\n",
    "\n",
    "print('<psi|H|psi> =', result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guide to Different Simulation Targets\n",
    "\n",
    "\n",
    "The figure below illustrates a few options for accelerating statevector simulations of single quantum processor kernel executions: one CPU, one GPU, or a multi-node, multi-GPU system. \n",
    "\n",
    "![](images/single-processor-backends.jpg)\n",
    "\n",
    "In the Hello World examples in the previous section, we saw statevector simulations of a QPU on a CPU.  When GPU resources are available, we can use a single-GPU or multi-node, multi-GPU systems for fast statevector simulations. The `nvidia` target accelerates statevector simulations through `cuStateVec` library. This target offers a variety of configuration options:\n",
    "\n",
    "* **Single-precision GPU simulation** (default): Setting the target to `nvidia` through command `cudaq.set_target('nvidia')` provides single (`fp32`) precision statevector simulation on one GPU as the default.\n",
    "\n",
    "* **Double fp64 precision on a single-GPU**: The option `cudaq.set_target('nvidia', option='fp64')` increases the precision of the statevector simulation on one GPU.\n",
    "\n",
    "* **Multi-node, multi-GPU simulation**: To run the `cuStateVec` simulator on multiple GPUs, set the target to `nvidia` with the `mgpu` option (`cudaq.set_target('nvidia', option='mgpu,fp64')`) and then run the python file containing your quantum kernels within a `MPI` context: `mpiexec -np 2 python3 program.py`. Adjust the `-np` tag according to the number of GPUs you have available.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll cover a few of the ways you can organize the distribution of quantum simulations over multiple GPU processors, whether you are simulating a single QPU or multiple QPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single QPU Statevector Simulations\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "In some cases, the memory required to hold the entire statevector exceeds the memory of a single GPU. In these cases we can distribute the statevector across multiple GPUs as the diagram in the image below suggests.  \n",
    "\n",
    "![](images/statevector-distribution.png)\n",
    "\n",
    "This is handled automatically within the `mgpu` option when the number of qubits in the statevector exceeds 25.  By changing the environmental variable `CUDAQ_MGPU_NQUBITS_THRESH` prior to setting the target, you can change the threshold in which statevector distribution is invoked.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simulating Parallel QPU computaiton\n",
    "\n",
    "There are several patterns for multi-QPU computation. We'll examine two of them here:\n",
    "\n",
    "* Circuit sampling distributed over multiple processors\n",
    "* Circuit cutting\n",
    "* Hamiltonian batching\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Circuit Sampling\n",
    "\n",
    "One method of parallelization is to sample a circuit over several processors as illustrated in the diagram below.\n",
    "\n",
    "![](images/circuit-sampling.png)\n",
    "\n",
    "The following code illustrates how to launch asynchronous sampling tasks using `sample_async` on multiple virtual QPUs, each simulated by a tensornet simulator backend using the `remote-mqpu` target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specified as program input, e.g.\n",
    "# ```\n",
    "# backend = \"tensornet\"; servers = \"2\"\n",
    "# ```\n",
    "backend = args.backend\n",
    "servers = args.servers\n",
    "\n",
    "# Define a kernel to be sampled.\n",
    "@cudaq.kernel\n",
    "def kernel(controls_count: int):\n",
    "    controls = cudaq.qvector(controls_count)\n",
    "    targets = cudaq.qvector(2)\n",
    "    # Place controls in superposition state.\n",
    "    h(controls)\n",
    "    for target in range(2):\n",
    "        x.ctrl(controls, targets[target])\n",
    "    # Measure.\n",
    "    mz(controls)\n",
    "    mz(targets)\n",
    "\n",
    "# Set the target to execute on and query the number of QPUs in the system;\n",
    "# The number of QPUs is equal to the number of (auto-)launched server instances.\n",
    "cudaq.set_target(\"remote-mqpu\",\n",
    "                backend=backend,\n",
    "                auto_launch=str(servers) if servers.isdigit() else \"\",\n",
    "                url=\"\" if servers.isdigit() else servers)\n",
    "qpu_count = cudaq.get_target().num_qpus()\n",
    "print(\"Number of virtual QPUs:\", qpu_count)\n",
    "\n",
    "# We will launch asynchronous sampling tasks,\n",
    "# and will store the results as a future we can query at some later point.\n",
    "# Each QPU (indexed by an unique Id) is associated with a remote REST server.\n",
    "count_futures = []\n",
    "for i in range(qpu_count):\n",
    "\n",
    "    result = cudaq.sample_async(kernel, i + 1, qpu_id=i)\n",
    "    count_futures.append(result)\n",
    "print(\"Sampling jobs launched for asynchronous processing.\")\n",
    "\n",
    "# Go do other work, asynchronous execution of sample tasks on-going.\n",
    "# Get the results, note future::get() will kick off a wait\n",
    "# if the results are not yet available.\n",
    "for idx in range(len(count_futures)):\n",
    "    counts = count_futures[idx].get()\n",
    "    print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hamiltonian Batching\n",
    "Another option for distributing the computation required in a simulation is through Hamiltonian batching in which expectation values of terms of the Hamiltonian are computed in parallel across multiple virtual QPUs as in the image below.\n",
    "\n",
    "![](images/Hamiltonian-batching.png)\n",
    "\n",
    "To distribute the expectation value computations of a multi-term Hamiltonian across multiple virtual QPUs use the `nvidia-mqpu` platform as in the following example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cudaq.set_target(\"nvidia\", option=\"mqpu\")\n",
    "target = cudaq.get_target()\n",
    "num_qpus = target.num_qpus()\n",
    "print(\"Number of QPUs:\", num_qpus)\n",
    "\n",
    "\n",
    "# Define spin ansatz.\n",
    "@cudaq.kernel\n",
    "def kernel(angle: float):\n",
    "    qvector = cudaq.qvector(2)\n",
    "    x(qvector[0])\n",
    "    ry(angle, qvector[1])\n",
    "    x.ctrl(qvector[1], qvector[0])\n",
    "\n",
    "\n",
    "# Define spin Hamiltonian.\n",
    "hamiltonian = 5.907 - 2.1433 * spin.x(0) * spin.x(1) - 2.1433 * spin.y(\n",
    "    0) * spin.y(1) + .21829 * spin.z(0) - 6.125 * spin.z(1)\n",
    "\n",
    "exp_val = cudaq.observe(kernel,\n",
    "                        hamiltonian,\n",
    "                        0.59,\n",
    "                        execution=cudaq.parallel.thread).expectation()\n",
    "print(\"Expectation value: \", exp_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code snippet, since the Hamiltonian contains four non-identity terms, there are four quantum circuits that need to be executed in order to compute the expectation value of that Hamiltonian and given the quantum state prepared by the ansatz kernel. When the nvidia-mqpu platform is selected, these circuits will be distributed across all available QPUs. The final expectation value result is computed from all QPU execution results.  \n",
    "\n",
    "Another option for Hamiltonian batching is to use the MPI context and multiple GPUs.  You can read more about this [here](https://nvidia.github.io/cuda-quantum/latest/using/backends/platform.html#nvidia-mqpu-platform)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Circuit cutting\n",
    "\n",
    "Circuit cutting is a common pattern for parallelization. One way of visualizing circuit cutting is through the max cut problem. In this example, we aim to approximate the max cut of a graph with the divide-and-conquer also referred to as QAOA-in-QAOA or QAOA^2 approach which breaks the graph down into smaller subgraphs and solves the max cuts for these subgraphs in parallel using QAOA (see for instance [arXiv:2205.11762v1](https://arxiv.org/abs/2205.11762), [arxiv.2101.07813v1](https://arxiv.org/abs/2101.07813), [arxiv:2304.03037v1](https://arxiv.org/abs/2304.03037), [arxiv:2009.06726](https://arxiv.org/abs/2009.06726), and [arxiv:2406:17383](https://arxiv.org/abs/2406.17383)).  By doing this we have effectively cut the QAOA circuit for the larger graph into the smaller QAOA for circuits for the subgraphs.  To complete the circuit cutting, we'll need to merge the results of QAOA on the subgraphs into a result for the entire graph.  This requires solving another smaller optimization problem, which can also be tackled with QAOA.  You can read it about that in more detail in a series of [interactive labs](https://github.com/NVIDIA/cuda-q-academic/tree/main/qaoa-for-max-cut).\n",
    "\n",
    "![](images/circuit-cutting.png)\n",
    "\n",
    "This example illustrates how to use the `MPI` context to orchestrate running `@cudaq.kernel` decorated functions in parallel. Additionally, a few exercises are built into this longer example to gain some practice with the CUDA-Q commands introduced earlier in this notebook. Solutions to these exercises appear in the solutions.ipynb file, but we encourage you to first attempt the exercises out yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to define a graph and subgraphs.  Execute the cell below to generate the graph and subgraphs for the divide-and-conquer QAOA. For this demonstrate, we'll stick with a small graph and divide it into five smaller graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an example graph\n",
    "edgeList = [(0,1), (1,2), (2,3), (3,0), (0,2), (2,4), (3,4), (4,5), (3,5)]\n",
    "\n",
    "# Identify subgraphs, separating out the edges as source and target nodes\n",
    "nodeCountList = [8,7,6,5,4]\n",
    "nodeList : List[int] = []\n",
    "edgeListSources : List[int] = []\n",
    "edgeListTargets : List[int] = []\n",
    "\n",
    "# subgraph0 data\n",
    "nodeList.append([3, 6, 9, 10, 13, 14, 21, 22])\n",
    "edgeListSources.append([3,3,3,3,6,6,9,14])\n",
    "edgeListTargets.append([14,9,10,13,22,13,21,22])\n",
    "\n",
    "# subgraph1 data\n",
    "nodeList.append([8, 11, 12, 15, 16, 25, 26])\n",
    "edgeListSources.append([8, 8, 11, 11, 11, 11, 12, 15, 16, 16, 25])\n",
    "edgeListTargets.append([25, 12, 26, 25, 15, 12, 15, 16, 25, 26, 26])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's create kernels for the QAOA circuit.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem Kernel\n",
    "\n",
    "@cudaq.kernel\n",
    "def qaoaProblem(qubit_0 : cudaq.qubit, qubit_1 : cudaq.qubit, alpha : float):\n",
    "    \"\"\"Build the QAOA gate sequence between two qubits that represent an edge of the graph\n",
    "    Parameters\n",
    "    ----------\n",
    "    qubit_0: cudaq.qubit\n",
    "        Qubit representing the first vertex of an edge\n",
    "    qubit_1: cudaq.qubit\n",
    "        Qubit representing the second vertex of an edge\n",
    "    alpha: float\n",
    "        Free variable\n",
    "\n",
    "    \"\"\"\n",
    "    x.ctrl(qubit_0, qubit_1)\n",
    "    rz(2.0*alpha, qubit_1)\n",
    "    x.ctrl(qubit_0, qubit_1)\n",
    "\n",
    "# Mixer Kernel\n",
    "@cudaq.kernel\n",
    "def qaoaMixer(qubit_0 : cudaq.qubit, beta : float):\n",
    "    \"\"\"Build the QAOA gate sequence that is applied to each qubit in the mixer portion of the circuit\n",
    "    Parameters\n",
    "    ----------\n",
    "    qubit_0: cudaq.qubit\n",
    "        Qubit\n",
    "    beta: float\n",
    "        Free variable\n",
    "\n",
    "    \"\"\"\n",
    "    rx(2.0*beta, qubit_0)\n",
    "\n",
    "\n",
    "# We now define the kernel_qaoa function which will be the QAOA circuit for our graph\n",
    "@cudaq.kernel\n",
    "def kernel_qaoa(qubit_count :int, layer_count: int, edges_src: List[int], edges_tgt: List[int], nodes: List[int], thetas : List[float]):\n",
    "    \"\"\"Build the QAOA circuit for max cut of the graph with given edges and nodes\n",
    "    Parameters\n",
    "    ----------\n",
    "    qubit_count: int\n",
    "        Number of qubits in the circuit, which is the same as the number of nodes in our graph\n",
    "    layer_count : int\n",
    "        Number of layers in the QAOA kernel\n",
    "    edges_src: List[int]\n",
    "        List of the first (source) node listed in each edge of the graph, when the edges of the graph are listed as pairs of nodes\n",
    "    edges_tgt: List[int]\n",
    "        List of the second (target) node listed in each edge of the graph, when the edges of the graph are listed as pairs of nodes\n",
    "    thetas: List[float]\n",
    "        Free variables to be optimized\n",
    "    \"\"\"\n",
    "    # Let's allocate the qubits\n",
    "    qreg = cudaq.qvector(qubit_count)\n",
    "\n",
    "    # And then place the qubits in superposition\n",
    "    h(qreg)\n",
    "    \n",
    "    # Each layer has two components: the problem kernel and the mixer\n",
    "    for i in range(layer_count):\n",
    "        # Add the problem kernel to each layer\n",
    "        for edge in range(len(edges_src)):\n",
    "            qubitu = nodes.index(edges_src[edge])\n",
    "            qubitv = nodes.index(edges_tgt[edge])\n",
    "            qaoaProblem(qreg[qubitu], qreg[qubitv], thetas[i])\n",
    "        # Add the mixer kernel to each layer\n",
    "        for j in range(qubit_count):\n",
    "            qaoaMixer(qreg[j],thetas[i+layer_count])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need a Hamiltonian to encode the cost function.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to generate the Hamiltonian for a max cut problem using the graph G\n",
    "\n",
    "def hamiltonian_max_cut(nodes : List[int], sources : List[int], targets : List[int]):\n",
    "    \"\"\"Hamiltonian for finding the max cut for the graph  with edges defined by the pairs generated by source and target edges\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    nodes : List[int]\n",
    "        list of nodes the nodes in the subgraph\n",
    "    sources: List[int]\n",
    "        list of the source vertices for edges in the graph\n",
    "    targets: List[int]\n",
    "        list of the target vertices for the edges in the graph\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cudaq.SpinOperator\n",
    "        Hamiltonian for finding the max cut of the graph defined by the given edges\n",
    "    \"\"\"\n",
    "    hamiltonian = 0\n",
    "   \n",
    "    # Since our vertices may not be a list from 0 to n, or may not even be integers,\n",
    "    # we need to map the vertices to the list of integers 0 to qubit_count -1\n",
    "    \n",
    "    for i in range(len(sources)):\n",
    "        # Add a term to the Hamiltonian for the edge (u,v)\n",
    "        qubitu = nodes.index(sources[i])\n",
    "        qubitv = nodes.index(targets[i])\n",
    "        hamiltonian += 0.5*(spin.z(qubitu)*spin.z(qubitv)-spin.i(qubitu)*spin.i(qubitv))\n",
    "\n",
    "    return hamiltonian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's put this all together in a function that finds the the optimal parameters for QAOA of a given subgraph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running this function in parallel, let's execute it sequentially. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll need to sample the `kernel_qaoa` circuit with the optimal parameters to find approximate max cut solutions to each of the subgraphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn more about how the results of the subgraph solutions are merged together to get a max cut approximation of the original graph, check out the 2nd notebook of this [series of interactive tutorials](https://github.com/NVIDIA/cuda-q-academic/tree/main/qaoa-for-max-cut)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To execute the workflow below on 4 processors (these can be separate GPUs or separate processes on a single GPU), we'll use MPI and the `rank` variable.\n",
    "\n",
    "![](images/parallel-workflow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beyond Statevector Simulations\n",
    "\n",
    "#### Other simulators\n",
    "\n",
    "If an NVIDIA GPU and CUDA runtime libraries are available, the default target is set to `nvidia`. This will utilize the cuQuantum single-GPU state vector simulator. On CPU-only systems, the default target is set to `qpp-cpu` which uses the OpenMP CPU-only simulator.\n",
    "\n",
    "For many applications, it's not necessary to simluate and access the entire statevector. The default simulator can be overridden by the environment variable CUDAQ_DEFAULT_SIMULATOR where tensor network, matrix product state simulators can be selected. Please refer to the table below for a list of backend simulator names along with its multi-GPU capability.\n",
    "\n",
    "![](images/backends.png)\n",
    "\n",
    "For more information about all the simulator backends available on [this documentation page](https://nvidia.github.io/cuda-quantum/latest/using/backends/simulators.html#tensor-network-simulators).\n",
    "\n",
    "#### Quantum processing units\n",
    "In addition to executing simulations, CUDA-Q is equipped to run quantum kernels on quantum processing units.  For more information on how to execute CUDA-Q code on quantum processing units, check out the [documentation](https://nvidia.github.io/cuda-quantum/latest/using/backends/hardware.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next up:\n",
    "[Installing CUDA-Q locally](https://nvidia.github.io/cuda-quantum/latest/using/quick_start.html#install-cuda-q) and [accessing the NVQC](https://nvidia.github.io/cuda-quantum/latest/using/backends/nvqc.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
